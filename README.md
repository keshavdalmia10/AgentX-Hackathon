# AgentX: Multi-Dialect SQL Evaluation Framework

> A comprehensive evaluation framework for LLM-powered SQL agents with multi-dialect support, hallucination detection, and multi-dimensional scoring.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

---

## Overview

AgentX evaluates SQL queries generated by LLM agents across multiple database dialects. It provides:

- **Multi-Dialect Support**: SQLite, DuckDB, PostgreSQL, BigQuery, Snowflake
- **Hallucination Detection**: Identifies phantom tables, columns, and functions before execution
- **Enhanced Scoring**: 7-dimensional scoring with adaptive thresholds
- **Dialect-Aware Validation**: Validates SQL syntax and functions per dialect

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          AGENTX EVALUATION SYSTEM                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                         SQL EXECUTOR                                  │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  │  │
│  │  │   SQLite    │  │   DuckDB    │  │ PostgreSQL  │  │  BigQuery   │  │  │
│  │  │  (default)  │  │ (analytics) │  │   (prod)    │  │  (cloud)    │  │  │
│  │  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘  │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                        │
│                                    ▼                                        │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                      VALIDATION LAYER                                 │  │
│  │  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────────┐   │  │
│  │  │ SQL Parser      │  │ Hallucination   │  │ Schema Validator    │   │  │
│  │  │ • sqlglot AST   │  │ Detector        │  │ • Table existence   │   │  │
│  │  │ • Multi-dialect │  │ • Phantom tables│  │ • Column validation │   │  │
│  │  │ • Transpilation │  │ • Phantom cols  │  │ • Type checking     │   │  │
│  │  └─────────────────┘  │ • Invalid funcs │  └─────────────────────┘   │  │
│  │                       └─────────────────┘                             │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                        │
│                                    ▼                                        │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                       SCORING ENGINE                                  │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  │  │
│  │  │ Correctness │  │ Efficiency  │  │   Safety    │  │  Semantic   │  │  │
│  │  │    35%      │  │    15%      │  │    20%      │  │  Accuracy   │  │  │
│  │  │             │  │  Adaptive   │  │  Weighted   │  │    10%      │  │  │
│  │  └─────────────┘  │ Thresholds  │  │ Hallucin.   │  └─────────────┘  │  │
│  │                   └─────────────┘  └─────────────┘                    │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                    │  │
│  │  │Completeness │  │    Best     │  │    Plan     │                    │  │
│  │  │    10%      │  │  Practices  │  │  Quality    │                    │  │
│  │  │             │  │     5%      │  │     5%      │                    │  │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                    │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Quick Start

### Option 1: Docker (Recommended)

```bash
# Clone repository
git clone https://github.com/ashcastelinocs124/AgentX-Hackathon.git
cd AgentX-Hackathon

# Start with Docker (SQLite, zero config)
docker-compose up agentx

# Or build and run directly
docker build -t agentx-benchmark .
docker run -p 5000:5000 agentx-benchmark
```

The A2A server will be available at `http://localhost:5000`.

### Option 2: Local Installation

```bash
# Clone repository
git clone https://github.com/ashcastelinocs124/AgentX-Hackathon.git
cd AgentX-Hackathon

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```python
from agentx import SQLExecutor, ExecutorConfig

# SQLite (zero setup, in-memory)
executor = SQLExecutor(ExecutorConfig(dialect="sqlite"))

# Execute and evaluate a query
result = executor.process_query("SELECT 1 + 1 as answer")
print(f"Status: {result.overall_status}")  # SUCCESS or FAILED
print(f"Data: {result.data}")              # [{'answer': 2}]

executor.close()
```

### Run Evaluation Pipeline

```bash
# SQLite (default)
python run_evaluation_pipeline.py "SELECT * FROM users WHERE age > 25"

# DuckDB
python run_evaluation_pipeline.py "SELECT * FROM users" --dialect duckdb

# PostgreSQL
python run_evaluation_pipeline.py "SELECT * FROM users" \
    --dialect postgresql \
    --connection-string "postgresql://user:pass@localhost/db"

# With expected results for comparison
python run_evaluation_pipeline.py --file query.sql --expected expected.json
```

### Run Tests

```bash
# Test multi-dialect support
python test_multi_dialect.py

# Test enhanced scoring
python test_enhanced_scoring.py

# Test evaluation pipeline integration
python test_evaluation_pipeline.py

# Test A2A protocol interface
python test_a2a.py
```

---

## Supported Dialects

| Dialect | Status | Setup Required | Use Case |
|---------|--------|----------------|----------|
| **SQLite** | Default | None | Testing, development, small datasets |
| **DuckDB** | Supported | `pip install duckdb` | Analytics, columnar workloads |
| **PostgreSQL** | Supported | Connection string | Production databases |
| **BigQuery** | Supported | GCP credentials | Cloud data warehouses |
| **Snowflake** | Planned | - | Enterprise data warehouses |
| **MySQL** | Planned | - | Web applications |

---

## Scoring Dimensions

### Enhanced Scorer (7 Dimensions)

| Dimension | Weight | Description |
|-----------|--------|-------------|
| **Correctness** | 35% | Result matches expected output |
| **Efficiency** | 15% | Query execution time (adaptive thresholds) |
| **Safety** | 20% | Validation + weighted hallucination severity |
| **Completeness** | 10% | Result quality (nulls, truncation) |
| **Semantic Accuracy** | 10% | Value-level comparison beyond row counts |
| **Best Practices** | 5% | SQL code quality (no SELECT *, proper JOINs) |
| **Plan Quality** | 5% | Execution plan analysis (index usage) |

### Scoring Presets

```python
from evaluation.enhanced_scorer import create_enhanced_scorer

# Balanced scoring (default)
scorer = create_enhanced_scorer("default")

# Higher weight on correctness and safety
scorer = create_enhanced_scorer("strict")

# Higher weight on efficiency and plan quality
scorer = create_enhanced_scorer("performance")

# Higher weight on best practices
scorer = create_enhanced_scorer("quality")
```

---

## Key Features

### 1. Hallucination Detection

Detects phantom identifiers **before** execution:

```python
from agentx import HallucinationDetector, create_adapter

# Create schema snapshot
adapter = create_adapter("sqlite")
adapter.connect()
adapter.execute("CREATE TABLE users (id INT, name TEXT)")
schema = adapter.get_schema_snapshot()

# Detect hallucinations
detector = HallucinationDetector(dialect="sqlite")
report = detector.detect("SELECT fake_column FROM users", schema)

print(report.phantom_columns)  # ['fake_column']
print(report.hallucination_score)  # 0.5
```

### 2. Query Complexity Analysis

Adaptive expectations based on query complexity:

```python
from evaluation.advanced_scoring import QueryComplexityAnalyzer

analyzer = QueryComplexityAnalyzer()

# Simple query
report = analyzer.analyze("SELECT name FROM users WHERE id = 1")
print(report.complexity_level)  # "simple"

# Complex query
report = analyzer.analyze("""
    WITH monthly AS (SELECT user_id, SUM(amount) FROM orders GROUP BY 1)
    SELECT u.name, m.total, ROW_NUMBER() OVER (ORDER BY m.total)
    FROM users u JOIN monthly m ON u.id = m.user_id
""")
print(report.complexity_level)  # "complex" or "very_complex"
```

### 3. Dialect-Aware Validation

Validates functions per dialect:

```python
from agentx import MultiDialectSQLParser

parser = MultiDialectSQLParser()

# SAFE_DIVIDE is BigQuery-specific
invalid = parser.validate_functions(
    "SELECT SAFE_DIVIDE(a, b) FROM table1",
    dialect="sqlite"
)
print(invalid)  # ['SAFE_DIVIDE']

# Valid for BigQuery
invalid = parser.validate_functions(
    "SELECT SAFE_DIVIDE(a, b) FROM table1",
    dialect="bigquery"
)
print(invalid)  # []
```

### 4. Result Comparison

Flexible comparison with tolerance:

```python
from evaluation.result_comparator import DefaultResultComparator

comparator = DefaultResultComparator(
    numeric_tolerance=1e-6,
    ignore_row_order=True,
    case_sensitive=False,
)

result = comparator.compare(actual_results, expected_results)
print(f"Match: {result.is_match}")
print(f"Score: {result.match_score}")
```

---

## A2A Protocol Interface

AgentX provides a standardized A2A (Agent-to-Agent) REST API for external agents to interact with the benchmark.

### Start the A2A Server

```bash
# Start with default settings (SQLite, port 5000)
python -m a2a.server

# Custom configuration
python -m a2a.server --dialect postgresql --port 8080 --host 0.0.0.0
```

### API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/info` | Get benchmark information |
| `GET` | `/health` | Health check |
| `GET` | `/schema` | Get database schema |
| `POST` | `/agents/register` | Register an agent |
| `POST` | `/tasks` | Get available evaluation tasks |
| `POST` | `/evaluate` | Submit SQL for evaluation |
| `POST` | `/evaluate/batch` | Batch evaluation |
| `GET` | `/leaderboard` | View leaderboard |
| `GET` | `/agents/{id}/results` | Get agent results |

### Agent Integration Example

```python
from a2a import A2AClient

# Connect to benchmark server
client = A2AClient("http://localhost:5000")

# Register your agent
agent = client.register(
    agent_name="MyLLMAgent",
    agent_version="1.0.0",
    capabilities=["sql_generation", "schema_understanding"],
)

# Get available tasks
tasks = client.get_tasks(difficulty="easy", limit=5)

# Submit SQL for evaluation
for task in tasks:
    sql = my_llm_generate_sql(task.question, task.schema_info)  # Your agent
    result = client.evaluate(task.task_id, sql)

    print(f"Task: {task.task_id}")
    print(f"Score: {result.scores.overall:.2%}")
    print(f"Correctness: {result.scores.correctness:.2%}")
    print(f"Safety: {result.scores.safety:.2%}")

# Check your ranking
leaderboard = client.get_leaderboard()
```

### Evaluation Request Format

```json
{
    "agent_id": "your-agent-id",
    "task_id": "sqlite_simple_select",
    "sql": "SELECT * FROM customers LIMIT 10",
    "execution_trace": [
        {"step": 1, "action": "analyze_schema"},
        {"step": 2, "action": "generate_sql"}
    ]
}
```

### Evaluation Response Format

```json
{
    "task_id": "sqlite_simple_select",
    "status": "success",
    "scores": {
        "overall": 0.95,
        "correctness": 1.0,
        "efficiency": 0.9,
        "safety": 1.0,
        "completeness": 0.9,
        "semantic_accuracy": 1.0,
        "best_practices": 0.85,
        "plan_quality": 0.95
    },
    "execution_success": true,
    "rows_returned": 10,
    "execution_time_ms": 2.5,
    "suggestions": ["Specify only the columns you need"]
}
```

---

## Benchmark Runner

AgentX includes a comprehensive benchmark runner for batch evaluation with metrics export.

### Run Benchmarks

```bash
# Run all benchmarks with default settings
python run_benchmark.py --output results/

# Filter by difficulty level
python run_benchmark.py --difficulty easy,medium --output results/

# Filter by tags
python run_benchmark.py --tags join,aggregation --output results/

# Export in multiple formats
python run_benchmark.py --format json,csv,summary,html --output results/

# Custom task file
python run_benchmark.py --tasks path/to/tasks.json --output results/
```

### Output Formats

| Format | Description | File |
|--------|-------------|------|
| `json` | Full structured report with all details | `benchmark_{id}.json` |
| `csv` | Tabular results for spreadsheets | `benchmark_{id}.csv` |
| `summary` | Human-readable text summary | `benchmark_{id}_summary.txt` |
| `html` | Interactive HTML report with visualizations | `benchmark_{id}.html` |

### Example Output

```
============================================================
AGENTX BENCHMARK RUN: bedcb09c
============================================================
Tasks: 17
Difficulties: ['easy', 'medium']
Dialect: sqlite
============================================================

[1/17] sqlite_simple_select (easy)... ✓ 87.6%
[2/17] sqlite_count (easy)... ✓ 77.1%
[3/17] sqlite_join (medium)... ✓ 99.8%
...

RESULTS SUMMARY
----------------------------------------
Total Tasks:  17
Successful:   16
Failed:       1

SCORES
----------------------------------------
Average:  94.40%
Median:   96.00%
Min:      77.10%
Max:      100.00%

SCORES BY DIMENSION
----------------------------------------
  correctness         : 86.82%
  efficiency          : 100.00%
  safety              : 100.00%

SCORES BY DIFFICULTY
----------------------------------------
  easy        : 92.65% (n=10)
  medium      : 97.31% (n=6)
```

### Programmatic Usage

```python
from run_benchmark import BenchmarkRunner, BenchmarkConfig, MetricsExporter

# Configure benchmark
config = BenchmarkConfig(
    output_dir="results",
    difficulties=["easy", "medium"],
    formats=["json", "csv", "html"],
    dialect="sqlite",
)

# Run benchmark
runner = BenchmarkRunner(config)
report = runner.run()

# Export results
exporter = MetricsExporter(config.output_dir)
outputs = exporter.export(report, config.formats)

# Access results programmatically
print(f"Average Score: {report.average_score:.2%}")
print(f"Successful: {report.successful}/{report.total_tasks}")

for result in report.results:
    print(f"{result.task_id}: {result.overall_score:.2%}")
```

### Custom Agent Integration

```python
def my_llm_generate_sql(task):
    """Your LLM agent that generates SQL from task description."""
    # Call your LLM here
    return f"SELECT * FROM customers"

# Run benchmark with your agent
runner = BenchmarkRunner(config)
report = runner.run(sql_generator=my_llm_generate_sql)
```

---

## Enterprise Benchmark

AgentX includes a comprehensive enterprise benchmark with realistic data warehouse schemas and complex SQL patterns.

### Enterprise Schema

The enterprise schema includes 19 tables modeling a realistic data warehouse:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         ENTERPRISE DATA WAREHOUSE                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  FACT TABLES                         DIMENSION TABLES                        │
│  ┌─────────────────┐                 ┌─────────────────┐                    │
│  │ sales_fact      │────────────────▶│ dim_customer    │                    │
│  │ • 500 records   │                 │ • 50 customers  │                    │
│  │ • tenant_id     │                 │ • segments      │                    │
│  │ • measures      │                 └─────────────────┘                    │
│  └─────────────────┘                 ┌─────────────────┐                    │
│  ┌─────────────────┐                 │ dim_product     │                    │
│  │ orders_fact     │────────────────▶│ • 30 products   │                    │
│  │ • 300 orders    │                 │ • categories    │                    │
│  └─────────────────┘                 └─────────────────┘                    │
│  ┌─────────────────┐                 ┌─────────────────┐                    │
│  │ user_events     │                 │ dim_date        │                    │
│  │ • 1000 events   │                 │ • 365 days      │                    │
│  │ • funnel data   │                 │ • holidays      │                    │
│  └─────────────────┘                 └─────────────────┘                    │
│                                      ┌─────────────────┐                    │
│  SPECIAL TABLES                      │ dim_store       │                    │
│  ┌─────────────────┐                 │ • 5 stores      │                    │
│  │ dim_customer_scd│ (Type 2)        └─────────────────┘                    │
│  │ employees       │ (Hierarchy)     ┌─────────────────┐                    │
│  │ bill_of_materials│ (BOM)          │ dim_promotion   │                    │
│  │ tenants         │ (Multi-tenant)  │ • discounts     │                    │
│  └─────────────────┘                 └─────────────────┘                    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Run Enterprise Benchmark

```bash
# Run full enterprise benchmark
python run_benchmark.py --schema enterprise --output results/

# Run specific query patterns
python run_benchmark.py --schema enterprise --tags star_schema --output results/
python run_benchmark.py --schema enterprise --tags scd --output results/
python run_benchmark.py --schema enterprise --tags window --output results/
python run_benchmark.py --schema enterprise --tags recursive --output results/

# Export as HTML report
python run_benchmark.py --schema enterprise --format json,html --output results/
```

### Enterprise Query Patterns (30 Queries)

| Category | Queries | Tags |
|----------|---------|------|
| **Star Schema** | Fact-dimension joins, time-based analysis | `star_schema`, `fact_table`, `dimension` |
| **SCD Type 2** | Current records, history tracking, point-in-time | `scd`, `scd_type2`, `temporal` |
| **Window Functions** | Running totals, moving averages, LAG/LEAD, NTILE, PERCENT_RANK | `window`, `running_total`, `moving_average` |
| **Funnel Analysis** | Conversion funnels, drop-off rates | `funnel`, `conversion`, `marketing` |
| **Sessionization** | User session detection with inactivity timeout | `sessionization`, `user_behavior` |
| **Gap & Island** | Sequence gaps, consecutive streaks | `gaps_islands`, `streak`, `sequence` |
| **Recursive CTE** | Org hierarchy, BOM explosion | `recursive`, `hierarchy`, `bom` |
| **Analytics** | Cohort retention, RFM, churn prediction | `cohort`, `retention`, `churn` |
| **Inventory** | ABC classification, demand forecasting | `abc_analysis`, `inventory`, `forecasting` |
| **Data Quality** | Null checks, orphans, duplicates, anomalies | `data_quality`, `validation`, `anomaly` |
| **Multi-tenant** | Tenant isolation, cross-tenant analytics | `multi_tenant`, `saas` |
| **Attribution** | Multi-touch marketing attribution | `attribution`, `marketing` |

### Example Enterprise Queries

**Star Schema Join:**
```sql
SELECT dp.category, ds.region, SUM(sf.quantity * sf.unit_price) as total_sales
FROM sales_fact sf
JOIN dim_product dp ON sf.product_id = dp.product_id
JOIN dim_store ds ON sf.store_id = ds.store_id
GROUP BY dp.category, ds.region
ORDER BY total_sales DESC
```

**SCD Type 2 Point-in-Time:**
```sql
SELECT customer_id, customer_name, segment
FROM dim_customer_scd
WHERE valid_from <= '2024-06-15'
  AND (valid_to > '2024-06-15' OR valid_to IS NULL)
```

**Sessionization with 30-min Timeout:**
```sql
WITH time_diffs AS (
  SELECT user_id, event_timestamp,
    CASE WHEN (julianday(event_timestamp) -
               julianday(LAG(event_timestamp) OVER (PARTITION BY user_id ORDER BY event_timestamp))) * 24 * 60 > 30
         THEN 1 ELSE 0 END as new_session
  FROM user_events
)
SELECT user_id, SUM(new_session) OVER (PARTITION BY user_id ORDER BY event_timestamp) as session_id
FROM time_diffs
```

**Cohort Retention Analysis:**
```sql
WITH first_purchase AS (
  SELECT customer_id, strftime('%Y-%m', MIN(order_date)) as cohort_month
  FROM orders_fact GROUP BY customer_id
)
SELECT cohort_month, months_since_first,
  COUNT(DISTINCT customer_id) as active_customers,
  ROUND(COUNT(DISTINCT customer_id) * 100.0 / cohort_size, 2) as retention_rate
FROM ...
```

### Programmatic Enterprise Setup

```python
from tasks.enterprise_schema import setup_enterprise_schema
from agentx import SQLExecutor, ExecutorConfig

# Create executor
executor = SQLExecutor(ExecutorConfig(dialect="sqlite"))

# Setup enterprise schema with sample data
setup_enterprise_schema(executor)

# Now you have 19 tables with realistic data
result = executor.process_query("""
    SELECT dp.category, SUM(sf.quantity * sf.unit_price) as revenue
    FROM sales_fact sf
    JOIN dim_product dp ON sf.product_id = dp.product_id
    GROUP BY dp.category
""")
print(result.data)
```

---

## Project Structure

```
AgentX-Hackathon/
├── src/agentx/                    # Core evaluation library
│   ├── __init__.py                # Public API exports
│   ├── dialects/                  # Dialect configurations
│   │   └── registry.py            # SQLite, DuckDB, PostgreSQL, BigQuery configs
│   ├── executor/                  # SQL execution
│   │   └── sql_executor.py        # Multi-dialect executor
│   ├── infrastructure/            # Database adapters
│   │   ├── database.py            # SQLite, DuckDB, PostgreSQL adapters
│   │   └── models.py              # Schema models (TableInfo, ColumnInfo)
│   └── validation/                # SQL validation
│       ├── sql_parser.py          # Multi-dialect parser (sqlglot)
│       └── hallucination.py       # Hallucination detection
│
├── a2a/                           # A2A Protocol Interface
│   ├── __init__.py                # Public exports
│   ├── server.py                  # REST API server (Flask)
│   ├── client.py                  # Client library for agents
│   └── models.py                  # Request/response models
│
├── evaluation/                    # Scoring system
│   ├── data_structures.py         # ExecutionResult, ComparisonResult
│   ├── result_comparator.py       # Result comparison logic
│   ├── scorer.py                  # Basic 4-dimension scorer
│   ├── advanced_scoring.py        # Advanced scoring components
│   └── enhanced_scorer.py         # 7-dimension enhanced scorer
│
├── docker/                        # Docker configuration
│   └── init-postgres.sql          # PostgreSQL initialization
├── Dockerfile                     # Container image definition
├── docker-compose.yml             # Multi-service orchestration
│
├── run_evaluation_pipeline.py     # CLI entry point
├── run_benchmark.py               # Benchmark runner with metrics export
├── requirements.txt               # Dependencies
│
├── tasks/                         # Benchmark tasks
│   ├── enterprise_schema.py       # Enterprise schema setup (19 tables)
│   └── gold_queries/              # Gold standard SQL queries
│       └── sqlite/
│           ├── basic_queries.json      # 27 basic tasks
│           └── enterprise_queries.json # 30 enterprise tasks
│
├── results/                       # Benchmark output directory
│
├── test_multi_dialect.py          # Multi-dialect tests
├── test_enhanced_scoring.py       # Scoring component tests
├── test_evaluation_pipeline.py    # Integration tests
└── test_a2a.py                    # A2A interface tests
```

---

## API Reference

### SQLExecutor

```python
from agentx import SQLExecutor, ExecutorConfig

# Configuration
config = ExecutorConfig(
    dialect="sqlite",           # sqlite, duckdb, postgresql, bigquery
    db_path=":memory:",         # For file-based DBs
    connection_string=None,     # For PostgreSQL
    row_limit=1000,             # Max rows to return
    timeout_seconds=30.0,       # Query timeout
    validate_before_execute=True,  # Enable hallucination detection
)

# Create executor
executor = SQLExecutor(config)

# Process query (validate -> execute -> analyze)
result = executor.process_query(sql, verbose=True)

# Access results
result.overall_status      # "SUCCESS" or "FAILED"
result.data                # List of row dicts
result.validation          # Validation details
result.execution           # Execution details
result.analysis            # Analysis insights

executor.close()
```

### EnhancedScorer

```python
from evaluation.enhanced_scorer import EnhancedScorer
from evaluation.data_structures import ComparisonResult, ExecutionResult

scorer = EnhancedScorer()

score = scorer.score(
    comparison=comparison_result,
    execution_result=execution_result,
    sql="SELECT ...",           # For complexity analysis
    dialect="sqlite",           # For adaptive thresholds
    expected_results=[...],     # For semantic accuracy
    plan_text="...",            # For plan analysis
)

# Access scores
score.overall               # 0.0 - 1.0
score.correctness           # 0.0 - 1.0
score.efficiency            # 0.0 - 1.0
score.safety                # 0.0 - 1.0
score.semantic_accuracy_score
score.best_practices_score
score.plan_quality_score

# Detailed analysis
score.complexity_report     # Query complexity breakdown
score.hallucination_details # Hallucination penalties
score.error_analysis        # Error classification
```

---

## Configuration

### Docker Deployment

```bash
# SQLite (default, zero config)
docker-compose up agentx
# Server available at http://localhost:5000

# PostgreSQL (production)
docker-compose up agentx-postgres postgres
# Server available at http://localhost:5001

# DuckDB (analytics)
docker-compose up agentx-duckdb
# Server available at http://localhost:5002

# Run all tests in Docker
docker-compose run --rm test

# Stop all services
docker-compose down
```

### Docker Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `DIALECT` | `sqlite` | SQL dialect (sqlite, duckdb, postgresql) |
| `PORT` | `5000` | Server port |
| `HOST` | `0.0.0.0` | Server host |
| `PG_CONNECTION_STRING` | - | PostgreSQL connection string |

### Environment Variables (Local)

```bash
# PostgreSQL
export AGENTX_PG_HOST=localhost
export AGENTX_PG_PORT=5432
export AGENTX_PG_USER=user
export AGENTX_PG_PASSWORD=password
export AGENTX_PG_DATABASE=testdb

# BigQuery
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json
export AGENTX_BQ_PROJECT=my-project
export AGENTX_BQ_DATASET=my_dataset
```

### PostgreSQL with Docker

```bash
# Start PostgreSQL with sample data
docker-compose up postgres

# Connect to database
psql postgresql://agentx:agentx_password@localhost:5432/agentx_db
```

This starts PostgreSQL on `localhost:5432` with:
- User: `agentx`
- Password: `agentx_password`
- Database: `agentx_db`
- Sample tables: customers, orders, products, order_items

---

## Examples

### Example 1: Evaluate Query with Expected Results

```python
import sys
sys.path.insert(0, 'src')

from agentx import SQLExecutor, ExecutorConfig
from evaluation.enhanced_scorer import EnhancedScorer
from evaluation.result_comparator import DefaultResultComparator
from evaluation.data_structures import AgentResult

# Setup
executor = SQLExecutor(ExecutorConfig(dialect="sqlite"))
executor.adapter.execute("CREATE TABLE users (id INT, name TEXT, age INT)")
executor.adapter.execute("INSERT INTO users VALUES (1, 'Alice', 30), (2, 'Bob', 25)")
executor.refresh_schema()

# Execute query
result = executor.process_query("SELECT name, age FROM users WHERE age > 26")

# Compare with expected
expected = [{"name": "Alice", "age": 30}]
comparator = DefaultResultComparator()
comparison = comparator.compare(result.data, expected)

# Score
scorer = EnhancedScorer()
agent_result = AgentResult.from_agent_output(result.to_dict())
execution_result = agent_result.to_execution_result()

score = scorer.score(
    comparison=comparison,
    execution_result=execution_result,
    sql="SELECT name, age FROM users WHERE age > 26",
    dialect="sqlite",
    expected_results=expected,
)

print(f"Overall Score: {score.overall:.2%}")
print(f"Correctness: {score.correctness:.2%}")
print(f"Safety: {score.safety:.2%}")

executor.close()
```

### Example 2: Detect Hallucinations

```python
from agentx import SQLExecutor, ExecutorConfig

executor = SQLExecutor(ExecutorConfig(dialect="sqlite"))
executor.adapter.execute("CREATE TABLE products (id INT, name TEXT, price REAL)")
executor.refresh_schema()

# Query with phantom table
result = executor.process_query("SELECT * FROM fake_products")

print(f"Status: {result.overall_status}")  # FAILED
print(f"Errors: {result.validation['errors']}")
# ["Table 'fake_products' does not exist in schema"]

executor.close()
```

---

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests (`python test_multi_dialect.py && python test_enhanced_scoring.py`)
5. Commit (`git commit -m 'Add amazing feature'`)
6. Push (`git push origin feature/amazing-feature`)
7. Open a Pull Request

---

## License

MIT License - see [LICENSE](LICENSE) for details.

---

## References

- [sqlglot Documentation](https://sqlglot.com/) - SQL parser and transpiler
- [DuckDB Documentation](https://duckdb.org/docs/) - In-process analytics database
- [Spider 2.0 Paper](https://arxiv.org/abs/2411.07763) - SQL benchmark reference
